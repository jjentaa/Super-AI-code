{"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from google.colab import userdata\nusername = userdata.get('username')\nkey = userdata.get('key')\n!mkdir -p ~/.kaggle\n!echo '{{\"username\":\"{username}\",\"key\":\"{key}\"}}' > ~/.kaggle/kaggle.json\n!chmod 600 /root/.kaggle/kaggle.json","metadata":{"id":"NfkEipyZSMAI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!kaggle competitions download -c image-search -p Dataset\n!unzip \"Dataset/*.zip\" -d Dataset\n!rm Dataset/*.zip","metadata":{"id":"cChyAC_8Gpbp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Import zone","metadata":{"id":"NBNM4TyiH8HG"}},{"cell_type":"code","source":"import os\nimport pandas as pd\nfrom PIL import Image, ImageOps, ImageFilter\nimport torch\nfrom transformers import AutoModel, CLIPProcessor\nfrom tqdm import tqdm\nimport cv2\nimport numpy as np","metadata":{"id":"-OwsrnRYRL8Y"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Image preprocess","metadata":{"id":"cR_amPnaFH7v"}},{"cell_type":"code","source":"def process_image(image_path, model, processor):\n    image = Image.open(image_path).convert('RGB')\n    image = image.resize((224, 224))\n\n    #denoise\n    image = ImageOps.autocontrast(image)\n    image = cv2.fastNlMeansDenoisingColored(np.array(image),None,20,20,7,21)\n\n    image = Image.fromarray(image)\n\n    inputs = processor(images=image, return_tensors='pt').to('cuda')\n    outputs = model.get_image_features(inputs.pixel_values).cpu()\n\n    outputs = outputs / outputs.norm(p=2, dim=-1, keepdim=True)\n\n    return outputs","metadata":{"id":"azo4n6rIOVAW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def process_query_images(query_dir, model, processor):\n    query_images = []\n    query_classes = []\n\n    with torch.no_grad():\n        for file in os.listdir(query_dir):\n            image_path = os.path.join(query_dir, file)\n            outputs = process_image(image_path, model, processor)\n            query_images.append(outputs)\n            query_classes.append(int(file[:-4]))\n\n    query_images = torch.cat(query_images)\n    return query_images, query_classes","metadata":{"id":"YcgASiAYOWyE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def find_top_match(image_path, model, processor, query_images, query_classes):\n    image = Image.open(image_path).convert('RGB')\n\n    image = image.resize((224, 224))\n    image = image.filter(ImageFilter.BLUR)\n\n    image = cv2.fastNlMeansDenoisingColored(np.array(image),None,20,20,7,21)\n    image = Image.fromarray(image)\n\n    inputs = processor(images=image, return_tensors='pt').to('cuda')\n    outputs = process_image(image_path, model, processor)\n\n    cosine = torch.cosine_similarity(outputs, query_images)\n\n    if cosine.max() > 0.8351:\n        return query_classes[cosine.argmax().numpy().tolist()]\n    else:\n        return None","metadata":{"id":"uJYB8vWNOYQc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"src_dir = '/content/Dataset/test/images'\nquery_dir = '/content/Dataset/queries/queries'\n\nsubmission = pd.read_csv('/content/Dataset/sample_submission.csv')\n\nmodel_name = \"openai/clip-vit-large-patch14\"","metadata":{"id":"hQm4fNP-F7yR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = AutoModel.from_pretrained(model_name).cuda().eval()\nprocessor = CLIPProcessor.from_pretrained(model_name)","metadata":{"id":"T2r9AG58F-1e"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"query_images = []\nquery_classes = []\n\nwith torch.no_grad():\n  for file in os.listdir(query_dir):\n    image_path = os.path.join(query_dir, file)\n    outputs = process_image(image_path, model, processor)\n    query_images.append(outputs)\n    query_classes.append(int(file[:-4]))\n\n    query_images = torch.cat(query_images)","metadata":{"id":"jUF7s4cpF7Zd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission['cosine_class'] = 22\n\nfor idx, row in tqdm(submission.iterrows()):\n\n    image_path = os.path.join(src_dir, row['img_file'])\n    result_class = find_top_match(image_path, model, processor, query_images, query_classes)\n    if result_class:\n        submission.at[idx, 'cosine_class'] = result_class\n\n\nsub = submission[['img_file', 'cosine_class']]\nsub.columns = ['img_file', 'class']\nsub.to_csv('submission.csv', index=False)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jQdKiv5POcIs","outputId":"eb2f630b-fa9a-4a91-95cb-c3c04180b470"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":"`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"id2label\"]` will be overriden.\n\n`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"bos_token_id\"]` will be overriden.\n\n`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"eos_token_id\"]` will be overriden.\n\n1120it [08:13,  2.27it/s]\n"}]},{"cell_type":"code","source":"sub.iloc[5]","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CXJ_pfF0GE8X","outputId":"534af35e-ef26-4fb7-f32b-4d6bd683b6f9"},"execution_count":null,"outputs":[{"output_type":"execute_result","execution_count":36,"data":{"text/plain":["img_file    e7ceaec2-8a7c-45d4-a762-95d4456d4413.jpg\n","class                                             22\n","Name: 5, dtype: object"]},"metadata":{}}]},{"cell_type":"code","source":"sub","metadata":{"id":"O1wP2V1GgI7Z"},"execution_count":null,"outputs":[]}]}